{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions to build NN's with different features for testing\n",
    "Structure: Inputlayer-(FCL-ActivationLayer)-(FCL-ActivationLayer)...(FCL-ActivationLayer)-OutputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow\n",
    "np.random.seed(1)\n",
    "tensorflow.random.set_seed(1)\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, f1_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Class for layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    # computes the output Y of a layer for a given input X\n",
    "    def forward_propagation(self, input):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # computes dE/dX for a given dE/dY (and update parameters if any)\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for fully connected layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward propagation method:\n",
    "\n",
    "#### Input: \n",
    "$\\underline{x}$ - ROW vector of size $1 $ x $i$  \n",
    "    \n",
    "#### output of one layer:  \n",
    "  \n",
    "$ \\underline{y} =  \\underline{x \\cdot W} + \\underline{b}  $ - vector of size $j$ x $1$\n",
    "\n",
    "#### Used:\n",
    "     \n",
    "Weights: $W$ - matrix of size $i$ x $j$  \n",
    "bias: $ \\underline{b} $ COLUMN vector of length $j$ x $1$. One $b_j$ for each $y_j$   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward porpagation method:  \n",
    "  \n",
    "#### Input:  \n",
    "    \n",
    "$\\underline{output\\_error} = \\frac{d\\underline{E}}{d\\underline{y}} $ is a vector, of shape $1$ x $j$, with elements $\\frac{d\\underline{E}}{d\\underline{y_j}}$  \n",
    "  \n",
    "LearningRate $ \\alpha $  \n",
    "  \n",
    "$E$ is the loss function, calculated as (in class ...)  \n",
    "  \n",
    "\n",
    "#### Output:  \n",
    "  \n",
    "Weights_error = $ \\frac{d\\underline{E}}{d\\underline{W}} = \\underline{x}^T \\cdot \\frac{d\\underline{E}}{d\\underline{y}} $ is a matrix of size $i$ x $j$  \n",
    "\n",
    "#### Used:\n",
    "  \n",
    "$\\underline{bias\\_error} = output\\_error$ - vector length $1$ x $b$    \n",
    "  \n",
    "$ \\underline{input\\_error}: \\frac{d\\underline{E}}{d\\underline{x}} = \\frac{d\\underline{E}}{d\\underline{y}} \\cdot W^T $ - vector of size $1$ x $i$ \n",
    "  \n",
    "derivation on paper.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCLayer():\n",
    "    # input_size (i) = number of input neurons\n",
    "    # output_size (j) = number of output neurons\n",
    "    def __init__(self, input_size, output_size):\n",
    "        #start by setting random weights and biases\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.5\n",
    "\n",
    "    # returns y_vector for a given x_vector\n",
    "    def forward_propagation(self, input_data):\n",
    "        #reshape x into a row vector\n",
    "        self.input = input_data.reshape(1,-1)\n",
    "        #calculate output of layer: \n",
    "        # y = xw+b\n",
    "        #print (\"weight size \") \n",
    "        #print(self.weights.size) \n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    # computes dE/dW, dE/dB for a given output_error=dE/dY. \n",
    "    # Returns input_error=dE/dx.\n",
    "   \n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        # dE/dx - this is the output of this function\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        \n",
    "     \n",
    "        # (dE/dW = x^T * dE/dy)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        # db/dx = dE/dy = output_error\n",
    "\n",
    "        # update weights\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        #update biases\n",
    "        self.bias -= learning_rate * output_error\n",
    "        return input_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class for activation layer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### input:   \n",
    "activation function for the forward propagation  \n",
    "the derivative of the activation function for the backward propagation  \n",
    "\n",
    "### forward_propagation:  \n",
    "   \n",
    "input: $\\underline{y}$ of size $j$ x $1$  \n",
    "output: $ \\underline{y_{act}} $  of size $j$ x $1$\n",
    "  \n",
    "### backward_propagation: \n",
    "   \n",
    "input: $ \\frac{d\\underline{E}}{d\\underline{y}} $ of size $1$ x $j$  \n",
    "output: $ \\frac{d\\underline{E}}{d\\underline{y_{act}}} * f'(\\underline{y_{act}}) $ of size $1$ x $j$  \n",
    "this is element-wise multiplication, so $\\frac{d\\underline{E}}{d{y_{act, k}}}$ * $f'(y_{act, k})$ etc.  \n",
    "the derivative of the activation function is with respect to its input, in this case $y_{act}$ (see calculation below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward_propagation(self, input_data):\n",
    "        self.input = input_data\n",
    "        self.output = self.activation(self.input)\n",
    "        # y activated\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error for a given output_error\n",
    "    # learning_rate is not used because the learnable parameters are in the FC layers\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        #element-wise multiplication\n",
    "        return self.activation_prime(self.input) * output_error"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation functions and their derivatives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$  \n",
    "  \n",
    "$\\tanh'(x) = \\frac{{(e^x+e^{-x})(e^x+e^{-x})-(e^x-e^{-x})(e^x-e^{-x})}}{{(e^x+e^{-x})^2}} = \\frac{{(e^x+e^{-x})^2-(e^x-e^{-x})^2}}{{(e^x+e^{-x})^2}}= 1-\\frac{{(e^x-e^{-x})^2}}{{(e^x+e^{-x})^2}} = 1 - \\tanh^2(x)$  \n",
    "\n",
    "$relu'(0) = 1$ since $\\lim_{{x \\to 0^-}} = 1 $ and $ \\lim_{{x \\to 0^+}} = 0$ , we have to pick, by convention we pick 1  \n",
    "  \n",
    "$sigmoid(x) = \\frac{1}{1 + e^{-x}}$  \n",
    "  \n",
    "$sigmoid'(x) = \\frac{e^{-x}}{(e^{-x}+1)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return np.where(x > 0, 1, 0)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    sigmoid_x = sigmoid(x)\n",
    "    return sigmoid_x * (1 - sigmoid_x)\n",
    "\n",
    "activation_functions = [tanh, tanh_prime, relu, relu_prime]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function and its derivative  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-entropy\n",
    "$loss(y_{true}, y_{pred}) = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_{true}^{(i)} \\log(y_{pred}^{(i)}) + (1 - y_{true}^{(i)}) \\log(1 - y_{pred}^{(i)}) \\right) $\n",
    "\n",
    "$\\frac{d loss(y_{true}, y_{pred})}{dy_{pred}}= \\frac{y_{pred} - y_{true}}{y_{pred} \\cdot (1 - y_{pred}) \\cdot j }$  \n",
    "  \n",
    "$j$ is the size of vector  $y$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15  # small constant to avoid division by zero\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # clip predictions to avoid numerical instability\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def cross_entropy_prime(y_true, y_pred):\n",
    "    epsilon = 1e-15  # small constant to avoid division by zero\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)  # clip predictions to avoid numerical instability\n",
    "    return (y_pred - y_true) / (y_pred * (1 - y_pred) * y_true.size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.loss_prime = None\n",
    "        self.configuration= None\n",
    "        self.act = None\n",
    "        self.act_prime = None\n",
    "        self.epoch_error = None\n",
    "      \n",
    "    \n",
    "  \n",
    "    def set_activation_index(self, a):\n",
    "        self.act = a\n",
    "    \n",
    "    def set_activation_prime_index(self, b):\n",
    "        self.act_prime = b\n",
    "\n",
    "    #add the configuration\n",
    "    def set_config(self, c):\n",
    "        self.configuration = c\n",
    "        \n",
    "    # add layer to network\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    # set up the loss functions\n",
    "    def use(self, loss, loss_prime):\n",
    "        self.loss = loss\n",
    "        self.loss_prime = loss_prime\n",
    "\n",
    "    # predict output for given input\n",
    "    def predict(self, input_data):\n",
    "        # sample dimension first\n",
    "        samples = len(input_data)\n",
    "\n",
    "        result = []\n",
    "\n",
    "        # run network over all samples\n",
    "        for i in range(samples):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "        return result\n",
    "\n",
    "    # train the network \n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        samples = len(x_train)\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(samples):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = self.loss_prime(y_train[j], output)\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "\n",
    "            # calculate average error on all samples\n",
    "            err /= samples\n",
    "            #print('epoch %d/%d   error=%f' % (i+1, epochs, err))\n",
    "            self.epoch_error = err\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different configurations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data input and management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hepatitis.data')\n",
    "\n",
    "# Define column names according to data description\n",
    "# Define column names according to data description\n",
    "column_names = ['Class', 'AGE', 'SEX', 'STEROID', 'ANTIVIRAL', 'FATIGUE', 'MALAISE', 'ANOREXIA', 'LIVER BIG', 'LIVER FIRM', 'SPLEEN PALPABLE', 'SPIDERS','ASCITES','VARICES', 'BILIRUBIN', 'ALK PHOSPHATE', 'SGOT', 'ALBUMINE', 'PROTIME', 'HISTOLOGY']\n",
    "\n",
    "# Load data with column names\n",
    "df = pd.read_csv('hepatitis.data', names=column_names)\n",
    "\n",
    "lrn = df.replace('?', np.nan)\n",
    "lrn = lrn.apply(pd.to_numeric)\n",
    "lrn.columns = lrn.columns.str.replace(' ', '')\n",
    "\n",
    "\n",
    "cats = ['Class', 'SEX', 'STEROID', 'ANTIVIRAL', 'FATIGUE', 'MALAISE', \n",
    "      'ANOREXIA', 'LIVERBIG', 'LIVERFIRM', 'SPLEENPALPABLE', \n",
    "     'SPIDERS', 'ASCITES', 'VARICES', 'HISTOLOGY']\n",
    "for i in cats:\n",
    "    lrn[i] = lrn[i].apply(lambda x: np.nan if pd.isnull(x) else x-1)\n",
    "\n",
    "\n",
    "y = lrn['Class']\n",
    "X = lrn.drop(['Class'], axis=1)\n",
    "X = X.dropna(axis='columns', how='all')\n",
    "X.columns[X.isnull().any()]\n",
    "X = X.fillna(X.mean())\n",
    "X[['STEROID', 'FATIGUE','ANOREXIA', 'MALAISE', 'LIVERBIG', 'LIVERFIRM',\n",
    "       'SPLEENPALPABLE', 'SPIDERS', 'ASCITES', 'VARICES', 'HISTOLOGY']] = X[['STEROID', 'FATIGUE','ANOREXIA', 'MALAISE', 'LIVERBIG', 'LIVERFIRM',\n",
    "       'SPLEENPALPABLE', 'SPIDERS', 'ASCITES', 'VARICES', 'HISTOLOGY']].round()\n",
    "\n",
    "x_total = X.to_numpy()\n",
    "y_total = y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y_total, test_size=0.1, random_state=1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.1, random_state=1)\n",
    "\n",
    "#scale\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "x_val = scaler.transform(x_val)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define metric F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1(actual_labels, predicted_values):\n",
    "    assert len(actual_labels) == len(predicted_values), \"Number of labels and predictions should match.\"\n",
    "    \n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    \n",
    "    for i in range(len(actual_labels)):\n",
    "        if actual_labels[i] == 1 and predicted_values[i] == 1:\n",
    "            true_positives += 1\n",
    "        elif actual_labels[i] == 0 and predicted_values[i] == 1:\n",
    "            false_positives += 1\n",
    "        elif actual_labels[i] == 1 and predicted_values[i] == 0:\n",
    "            false_negatives += 1\n",
    "    if true_positives + false_positives > 0:\n",
    "        precision = true_positives / (true_positives + false_positives)\n",
    "        recall = true_positives / (true_positives + false_negatives)\n",
    "    \n",
    "        f1 = 2 * (precision * recall) / (precision + recall)\n",
    "        return f1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the best structure for our network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10, 50, 100 or 200 nodes per layer  \n",
    "2, 3 or 4 hidden layers  \n",
    "0.3, 0.5 or 0.9 treshold  \n",
    "2 activation functions for hidden layers: tanh and relu  \n",
    "\n",
    "generate all combinations  \n",
    "calculate f1 for each  \n",
    "find best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_permutations(nums):\n",
    "    if len(nums) == 0:\n",
    "        return [[]]  # Base case: an empty list has one permutation, an empty list itself\n",
    "    \n",
    "    permutations = []\n",
    "    \n",
    "    for i in range(len(nums)):\n",
    "        current_num = nums[i]\n",
    "        remaining_nums = nums[:i] + nums[i+1:]\n",
    "        \n",
    "        for permutation in generate_permutations(remaining_nums):\n",
    "            permutations.append([current_num] + permutation)\n",
    "    \n",
    "    return permutations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_configurations(nodes_per_layer_options, layers):\n",
    "    configs = []\n",
    "    node_options = generate_permutations(nodes_per_layer_options)\n",
    "    for i in layers:\n",
    "        # Extract the first i columns and store unique values in a set\n",
    "        unique_values = set(tuple(row[:i]) for row in node_options)\n",
    "        # Convert the set back to a list of lists\n",
    "        extracted_columns = [list(values) for values in unique_values]\n",
    "        configs.append(extracted_columns)\n",
    "        #Print the extracted columns\n",
    "        #print(i, \"layers\")\n",
    "        #for column in extracted_columns:\n",
    "            #print( column)\n",
    "    return configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(nodes, activation_index, activation_prime_index):\n",
    "    net = Network()\n",
    "    net.set_activation_index(activation_index)\n",
    "    net.set_activation_prime_index(activation_prime_index)\n",
    "    net.add(FCLayer(x_total.shape[1], nodes[0]))  # input layer\n",
    "    print(\"input layer \", nodes[0] )\n",
    "\n",
    "    for i in range(len(nodes)-1):\n",
    "        net.add(FCLayer(nodes[i], nodes[i+1])) \n",
    "        print(\"hidden layer in\", nodes[i], \"out\", nodes[i+1] )\n",
    "        net.add(ActivationLayer(activation_functions[activation_index], activation_functions[activation_prime_index]))\n",
    " \n",
    "    net.add(FCLayer(nodes[-1], 1))  # output layer\n",
    "    net.add(ActivationLayer(softmax, softmax_prime))\n",
    "    print(\"output layer \", nodes[-1] )\n",
    "  \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_options = [10, 50, 100, 200]\n",
    "layer_options = [2, 3, 4]\n",
    "treshold = [0.5, 0.8, 0.9]\n",
    "configurations= create_configurations(nodes_options, layer_options)\n",
    "nns_tanh = []\n",
    "nns_relu = []\n",
    "data = []\n",
    "top = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tanh(c_i):\n",
    "    #activation_functions = [tanh, tanh_prime, relu, relu_prime]\n",
    "    for x in c_i:\n",
    "        c = create_network(x,0,1)\n",
    "        c.set_config(x)\n",
    "        nns_tanh.append(c)  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_relu (c_i):\n",
    "      for x in c_i:\n",
    "        c = create_network(x, 2, 3)\n",
    "        c.set_config(x)\n",
    "        nns_relu.append(c)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperparameter_search(nns):\n",
    "    epochs = [10, 25, 50]\n",
    "    for epoch in epochs:\n",
    "            for nets in nns:\n",
    "                nets.use(cross_entropy, cross_entropy_prime)\n",
    "                nets.fit(x_train, y_train, epochs=epoch, learning_rate=0.1)\n",
    "                pred = nets.predict(x_val)\n",
    "\n",
    "                # Convert the list of arrays to a numpy array\n",
    "                predicted = np.array([arr[0][0] for arr in pred])\n",
    "\n",
    "                for tresh in treshold:\n",
    "                    y_pred = np.where(predicted >= tresh, 1, 0)\n",
    "                    f1_s = f1_score(y_val*1, y_pred)\n",
    "                    top.append([nets.configuration, nets.act, nets.act_prime, tresh, epoch, f1_s])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n",
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\1171687839.py:14: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(layer_options)):\n",
    "   \n",
    "    generate_tanh(configurations[i])\n",
    "    generate_relu(configurations[i])\n",
    "\n",
    "    hyperparameter_search(nns_tanh)\n",
    "    hyperparameter_search(nns_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(top).to_csv('hepatitis-top2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 100, 50], 0, 1, 0.9, 10, 1.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_top = sorted(top, key=lambda x: x[-1], reverse=True)\n",
    "#sorted_top\n",
    "tops = sorted_top[0]\n",
    "tops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tops[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "#0 = config, 1= act, 2=act_prime, 3 = tresh, 4 = epochs\n",
    "mdl = create_network(tops[0], tops[1], tops[2])\n",
    "mdl.use(cross_entropy, cross_entropy_prime)\n",
    "mdl.fit(x_test, y_test, tops[4], 0.1)\n",
    "prd = mdl.predict(x_test)\n",
    "pr = np.array([arr[0][0] for arr in prd])\n",
    "bi = np.where(pr >= tops[3], 1, 0)\n",
    "f1_best = calculate_f1(y_test, bi)\n",
    "print(f1_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8148148148148148"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = create_network(tops[0], tops[1], tops[2])\n",
    "model.use(cross_entropy, cross_entropy_prime)\n",
    "model.fit(x_train, y_train, tops[4], 0.1)\n",
    "y_pred = model.predict(x_test)\n",
    "y_prd = np.array([arr[0][0] for arr in y_pred])\n",
    "y_pred = np.where(y_prd >= tops[3], 1, 0)\n",
    "f1_score(y_test*1, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def cross_validation(x, y, tops):\n",
    "    kfold = KFold(n_splits=5)\n",
    "    scores = []\n",
    "    # enumerate splits\n",
    "    for (train, test) in kfold.split(x,y):\n",
    "        x_tr = x[train]\n",
    "        y_tr = y[train]\n",
    "        x_tst = x[test]\n",
    "        y_tst = y[test]\n",
    "        model = create_network(tops[0], tops[1], tops[2])\n",
    "        model.use(cross_entropy, cross_entropy_prime)\n",
    "        model.fit(x_tr, y_tr, tops[4], 0.1)\n",
    "        y_pred = model.predict(x_tst)\n",
    "        y_prd = np.array([arr[0][0] for arr in y_pred])\n",
    "        y_pred = np.where(y_prd >= tops[3], 1, 0)\n",
    "        scores.append(f1_score(y_tst*1, y_pred))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9047619047619048, 0.7058823529411764, 0.7777777777777778, 0.8717948717948718, 0.9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8320433814551462"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = cross_validation(x_train, y_train, tops)\n",
    "print(scores)\n",
    "np.mean(scores)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN from library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import f1_score\n",
    "from keras.callbacks import Callback\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daria\\AppData\\Local\\Temp\\ipykernel_13040\\359195556.py:11: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  model = KerasClassifier(build_fn=create_model)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 2ms/step - loss: 0.6404 - accuracy: 0.6640\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "4/4 [==============================] - 1s 2ms/step - loss: 0.6249 - accuracy: 0.6600\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 0.6450 - accuracy: 0.6800\n",
      "4/4 [==============================] - 1s 2ms/step - loss: 0.6370 - accuracy: 0.6300\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4599 - accuracy: 0.8000\n",
      "4/4 [==============================] - 1s 2ms/step - loss: 0.5883 - accuracy: 0.7200\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 0.6294 - accuracy: 0.7600\n",
      "4/4 [==============================] - 1s 2ms/step - loss: 0.5835 - accuracy: 0.6900\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 0.6617 - accuracy: 0.8000\n",
      "4/4 [==============================] - 1s 2ms/step - loss: 0.6195 - accuracy: 0.6900\n",
      "1/1 [==============================] - 0s 100ms/step - loss: 0.4896 - accuracy: 0.7200\n",
      "0.9230769230769231\n"
     ]
    }
   ],
   "source": [
    "# Define the function to create the model\n",
    "def create_model(units = 200):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units, activation='tanh', input_dim=len(x_train[0])))\n",
    "    model.add(Dense(units, activation='tanh'))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create the KerasClassifier\n",
    "model = KerasClassifier(build_fn=create_model)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "cr = cross_validate(model, x_train, y_train, cv=5)\n",
    "\n",
    "f1 = calculate_f1(y_test, y_pred)\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7040000081062316"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cr['test_score'].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## extra: how we chose the learning-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 hand\n",
      "0.896551724137931\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#uncommented the print epoch error, see errors from epoch to epoch\n",
    "n= [10 ,20, 30]\n",
    "hand = create_network(n, tanh, tanh)\n",
    "hand.use(cross_entropy, cross_entropy_prime)\n",
    "\n",
    "\n",
    "# Define the range of learning rates to explore\n",
    "learning_rates = np.logspace(-6, 0, num=10)\n",
    "\n",
    "# Initialize lists to store learning rates and corresponding losses\n",
    "lr_values = []\n",
    "loss_values = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    \n",
    "    # Train your neural network for a few epochs with the current learning rate\n",
    "    hand.fit(x_train, y_train, 50, lr)\n",
    "    # Calculate and store the loss/error after each epoch\n",
    "    loss = hand.epoch_error\n",
    "    lr_values.append(lr)\n",
    "    loss_values.append(loss)\n",
    "\n",
    "# Plot the learning rate vs. loss curve\n",
    "plt.plot(lr_values, loss_values)\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.ylabel('Loss')\n",
    "plt.xscale('log')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "ph=hand.predict(x_test)\n",
    "# Convert the list of arrays to a numpy array\n",
    "predicted_ph = np.array([arr[0][0] for arr in ph])\n",
    "##add threshold\n",
    "binary_ph = np.where(predicted_ph >= 0.5, 1, 0)\n",
    "print(\"f1 hand\")\n",
    "f1 = calculate_f1(y_test, binary_ph)\n",
    "print(f1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
